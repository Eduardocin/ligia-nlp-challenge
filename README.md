# üîç Ligia NLP Challenge

> Solu√ß√£o completa de classifica√ß√£o de not√≠cias (Real vs Fake) usando Machine Learning cl√°ssico e Deep Learning com otimiza√ß√£o de hiperpar√¢metros.

Nota: Esta estrutura de projeto foi inicialmente gerada usando o template `cookiecutter-datascience`. Mantivemos a organiza√ß√£o e conven√ß√µes do template para facilitar reprodutibilidade, testes e contribui√ß√£o.

[![Python 3.10](https://img.shields.io/badge/python-3.10-blue.svg)](https://www.python.org/downloads/)
[![License](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE)
[![Template: cookiecutter-datascience](https://img.shields.io/badge/template-cookiecutter--datascience-orange.svg)](https://drivendata.github.io/cookiecutter-data-science/)

## üìã Descri√ß√£o

Este projeto implementa pipelines completos de NLP para classifica√ß√£o bin√°ria de not√≠cias, comparando abordagens cl√°ssicas (TF-IDF + ML tradicional) com t√©cnicas modernas. O projeto inclui an√°lise explorat√≥ria, pr√©-processamento avan√ßado, modelagem baseline, otimiza√ß√£o de hiperpar√¢metros e avalia√ß√£o comparativa de modelos.

## Estrutura do Projeto

```
‚îú‚îÄ‚îÄ LICENSE                  <- Licen√ßa do projeto
‚îú‚îÄ‚îÄ Makefile                 <- Comandos de conveni√™ncia (make data, make train)
‚îú‚îÄ‚îÄ README.md                <- Documenta√ß√£o principal para desenvolvedores
‚îú‚îÄ‚îÄ pyproject.toml           <- Configura√ß√£o do projeto e ferramentas (black, ruff)
‚îú‚îÄ‚îÄ requirements.txt         <- Depend√™ncias para reprodu√ß√£o do ambiente
‚îú‚îÄ‚îÄ setup.cfg                <- Configura√ß√£o de ferramentas de linting
‚îÇ
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ external/             <- Dados de fontes externas
‚îÇ   ‚îú‚îÄ‚îÄ interim/              <- Dados intermedi√°rios transformados
‚îÇ   ‚îú‚îÄ‚îÄ processed/            <- Datasets finais para modelagem
‚îÇ   ‚îî‚îÄ‚îÄ raw/                  <- Dados originais imut√°veis
‚îÇ
‚îú‚îÄ‚îÄ docs/                    <- Documenta√ß√£o do projeto (mkdocs)
‚îÇ
‚îú‚îÄ‚îÄ models/                  <- Modelos treinados, serializa√ß√µes e previs√µes
‚îÇ   ‚îî‚îÄ‚îÄ bert-baseline/        <- Checkpoints do modelo BERT fine-tuned
‚îÇ
‚îú‚îÄ‚îÄ notebooks/               <- Jupyter notebooks para an√°lise
‚îÇ   ‚îú‚îÄ‚îÄ 1.0-initial-data-exploration.ipynb     <- EDA completa
‚îÇ   ‚îú‚îÄ‚îÄ 2.0-text-preprocessing.ipynb           <- Limpeza de texto
‚îÇ   ‚îú‚îÄ‚îÄ 3.0-baseline-models-tfidf.ipynb        <- Modelos baseline
‚îÇ   ‚îú‚îÄ‚îÄ 3.1-hyperparameter-optimization.ipynb  <- Otimiza√ß√£o XGBoost/LinearSVC
‚îÇ   ‚îî‚îÄ‚îÄ 4.0-prediction.ipynb                   <- Predi√ß√µes finais
‚îÇ
‚îú‚îÄ‚îÄ references/              <- Dicion√°rios de dados, manuais e materiais explicativos
‚îÇ
‚îú‚îÄ‚îÄ reports/                 <- An√°lises geradas (HTML, PDF, LaTeX)
‚îÇ   ‚îî‚îÄ‚îÄ figures/              <- Gr√°ficos e figuras para relat√≥rios
‚îÇ
‚îî‚îÄ‚îÄ src/                     <- C√≥digo-fonte do projeto
    ‚îú‚îÄ‚îÄ __init__.py           <- Torna src um m√≥dulo Python
    ‚îú‚îÄ‚îÄ config.py             <- Vari√°veis e configura√ß√µes √∫teis
    ‚îú‚îÄ‚îÄ dataset.py            <- Scripts para download/gera√ß√£o de dados
    ‚îú‚îÄ‚îÄ features.py           <- Engenharia de features para modelagem
    ‚îú‚îÄ‚îÄ modeling/
    ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
    ‚îÇ   ‚îú‚îÄ‚îÄ predict.py        <- Infer√™ncia com modelos treinados
    ‚îÇ   ‚îî‚îÄ‚îÄ train.py          <- Treinamento de modelos
    ‚îî‚îÄ‚îÄ plots.py              <- Visualiza√ß√µes
```

## ‚öôÔ∏è Configura√ß√£o do Ambiente

### Op√ß√£o 1: Conda (Recomendado - Reprodutibilidade Garantida)

```bash
# Clonar reposit√≥rio
git clone https://github.com/seu-usuario/ligia-nlp-challenge.git
cd ligia-nlp-challenge

# Criar ambiente conda com todas as depend√™ncias
conda env create -f environment.yml

# Ativar ambiente
conda activate ligia-nlp

# Verificar instala√ß√£o
python -c "import sklearn, xgboost, transformers; print('‚úÖ Ambiente configurado com sucesso!')"

# Iniciar Jupyter Lab
jupyter lab
```

**Para atualizar o ambiente existente:**
```bash
conda env update -f environment.yml --prune
```

## Notebooks

Lista de notebooks principais (sem descri√ß√µes detalhadas):

- `notebooks/1.0-initial-data-exploration.ipynb`
- `notebooks/2.0-text-preprocessing.ipynb`
- `notebooks/3.0-baseline-models-tfidf.ipynb`
- `notebooks/3.1-hyperparameter-optimization.ipynb`
- `notebooks/4.0-prediction.ipynb`

## Pipeline de An√°lise

### 1. An√°lise Explorat√≥ria de Dados (EDA)
[notebooks/1.0-initial-data-exploration.ipynb](notebooks/1.0-initial-data-exploration.ipynb)
- Estat√≠sticas descritivas
- An√°lise de distribui√ß√£o de classes
- Visualiza√ß√£o de padr√µes textuais (wordclouds, n-gramas)
- Identifica√ß√£o de data leakage

### 2. Pr√©-processamento de Texto
[notebooks/2.0-text-preprocessing.ipynb](notebooks/2.0-text-preprocessing.ipynb)
- Remo√ß√£o de URLs, men√ß√µes, hashtags e HTML
- Tratamento de duplicatas
- Normaliza√ß√£o de texto
- Exporta√ß√£o de datasets limpos

### 2.1. Pr√©-processamento Avan√ßado com spaCy
[notebooks/2.1-spacy-text-preprocessing.ipynb](notebooks/2.1-spacy-text-preprocessing.ipynb)
- Lematiza√ß√£o com `en_core_web_sm`
- Extra√ß√£o de features lingu√≠sticas (POS ratios, NER counts)
- Texto lematizado para uso em TF-IDF e modelos

### 3. Modelos Baseline (TF-IDF)
[notebooks/3.0-baseline-models-tfidf.ipynb](notebooks/3.0-baseline-models-tfidf.ipynb)
- Representa√ß√£o TF-IDF (unigramas + bigramas)
- Random Forest Classifier
- XGBoost Classifier
- An√°lise de erros e features importantes

### 3.1. Baseline spaCy BERT (Feature Extraction)
[notebooks/3.1-spacy-bert-baseline.ipynb](notebooks/3.1-spacy-bert-baseline.ipynb)
- Embeddings RoBERTa-base via `en_core_web_trf` (768-d)
- Logistic Regression + MLP sobre embeddings
- CüöÄ Como Usar

### Reproduzir Pipeline Completo

```bash
# 1. Configurar ambiente
conda env create -f environment.yml
conda activate ligia-nlp

# 2. Executar notebooks na ordem
jupyter lab

# 3. Ordem de execu√ß√£o recomendada:
#    ‚Üí 1.0-initial-data-exploration.ipynb
#    ‚Üí 2.0-text-preprocessing.ipynb
#    ‚Üí 3.0-baseline-models-tfidf.ipynb
#    ‚Üí 3.1-hyperparameter-optimization.ipynb (otimiza√ß√£o completa ~1h)
#    ‚Üí 4.0-prediction.ipynb
```

### Usar Modelo Pr√©-treinado

```python
import joblib
import pandas as pd

# Carregar modelo e vetorizador
model = joblib.load('models/optimized/xgboost_optimized.joblib')
tfidf = joblib.load('models/optimized/tfidf_vectorizer.joblib')

# Fazer predi√ß√µes
texts = ["Exemplo de not√≠cia para classificar"]
X = tfidf.transform(texts)
predictions = model.predict(X)
print(f"Predi√ß√£o: {'Fake' if predictions[0] == 1 else 'Real'}")
```

## üìÇ Dados

- **Fonte:** Dataset de not√≠cias reais e falsas
- **Train:** `data/raw/train.csv` (~20k amostras)
- **Test:** `data/raw/test.csv` (~5k amostras)
- **Colunas:** `title`, `text`, `subject`, `label` (0=Real, 1=Fake)

## ü§ù Contribuindo

Contribui√ß√µes s√£o bem-vindas! Por favor:
1. Fork o projeto
2. Crie uma branch para sua feature (`git checkout -b feature/AmazingFeature`)
3. Commit suas mudan√ßas (`git commit -m 'Add some AmazingFeature'`)
4. Push para a branch (`git push origin feature/AmazingFeature`)
5. Abra um Pull Request

## üìù Licen√ßa

Este projeto est√° sob a licen√ßa especificada no arquivo [LICENSE](LICENSE).

## üë§ Autor

**Eduardo** - [GitHub](https://github.com/seu-usuario)

---

‚≠ê Se este projeto foi √∫til, considere dar uma estrela!

## Principais Tecnologias

- **Processamento de Dados:** pandas, numpy
- **Visualiza√ß√£o:** matplotlib, seaborn, wordcloud
- **NLP:** nltk
- **Machine Learning:** scikit-learn, xgboost
- **Otimiza√ß√£o:** RandomizedSearchCV, GridSearchCV
- **Ambiente:** Jupyter Lab/Notebook, conda

## Licen√ßa

Este projeto est√° sob a licen√ßa especificada no arquivo [LICENSE](LICENSE).
